## Where the samples will be written
save_data: run/test
## Where the vocab(s) will be written
src_vocab: run/test.vocab.src
tgt_vocab: run/test.vocab.tgt
# Prevent overwriting existing files in the folder
overwrite: False

# Corpus opts:
data:
    corpus_1:
        path_src: data/src/src-train.txt
        path_tgt: data/trg/trg-train.txt
    valid:
        path_src: data/src/src-val.txt
        path_tgt: data/trg/trg-val.txt

# General opts
save_model: run/luong/luong_attn
save_checkpoint_steps: 10000
valid_steps: 2000
train_steps: 40000

# Logging
report_every: 1000

# Batching
bucket_size: 32768 # A bucket is a buffer of bucket_size examples to pick. The dynamic iterator batches batch_size batchs from the bucket and shuffle them.
world_size: 1 # Total number of distributed processes.
gpu_ranks: [0] # List of ranks of each process.
num_workers: 4 # How many subprocesses to use for data loading. 0 means that the data will be loaded in the main process.
batch_type: "tokens" # Batch grouping for batch_size.
batch_size: 256 # Maximum batch size for training
valid_batch_size: 8 # Maximum batch size for validation
max_generator_batches: 2 # Not found in documentation
accum_count: [4] # Accumulate gradient this many times. Approximately equivalent to updating batch_size * accum_count batches at once.

# Optimization
model_dtype: "fp32" # Model datatype(fp16 or fp32)
optim: "adam" 
learning_rate: 0.001
# warmup_steps: 8000 # Number of warmup steps for custom decay.q
# decay_method: "noam" # The lr becomes too small almost instantly so comment this out for now
learning_rate_decay: 0.2 # If update_learning_rate, decay learning rate by this much if steps have gone past start_decay_steps
start_decay_steps: 20000 # Start decaying lr after X steps
decay_steps: 4000 # Decay lr after every X steps
max_grad_norm: 0
label_smoothing: 0.1 # Read here https://arxiv.org/abs/1512.00567 ?? :D ?
#param_init: 0
normalization: "tokens"

# Model
encoder_type: brnn
decoder_type: rnn
layers: 2
word_vec_size: 512
enc_rnn_size: 512
dec_rnn_size: 512
hidden_size: 512
rnn_type: GRU
dropout_steps: [0]
dropout: [0.1]

# Attention
global_attention: general # The attention type to use: dotprod or general (Luong) or MLP (Bahdanau)
attention_dropout: [0.1]
