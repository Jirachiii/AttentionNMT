## Where the samples will be written
save_data: run/test
## Where the vocab(s) will be written
src_vocab: run/test.vocab.src
tgt_vocab: run/test.vocab.tgt
# Prevent overwriting existing files in the folder
overwrite: true

# Corpus opts:
data:
    corpus_1:
        path_src: data/src/src-train.txt
        path_tgt: data/trg/trg-train.txt
    valid:
        path_src: data/src/src-val.txt
        path_tgt: data/trg/trg-val.txt

# General opts
save_model: multi_head_attn
save_checkpoint_steps: 10000
valid_steps: 2000
train_steps: 40000

# Logging
report_every: 1000

# Batching
bucket_size: 32768 # A bucket is a buffer of bucket_size examples to pick. The dynamic iterator batches batch_size batchs from the bucket and shuffle them.
world_size: 1 # Total number of distributed processes.
gpu_ranks: [0] # List of ranks of each process.
num_workers: 4 # How many subprocesses to use for data loading. 0 means that the data will be loaded in the main process.
batch_type: "tokens" # Batch grouping for batch_size.
batch_size: 256 # Maximum batch size for training
valid_batch_size: 8 # Maximum batch size for validation
max_generator_batches: 2 # Not found in documentation
accum_count: [4] # Accumulate gradient this many times. Approximately equivalent to updating batch_size * accum_count batches at once.

# Optimization
model_dtype: "fp32" # Model datatype(fp16 or fp32)
optim: "adam" 
learning_rate: 0.001
# warmup_steps: 8000 # Number of warmup steps for custom decay.q
# decay_method: "noam" # The lr becomes too small almost instantly so comment this out for now
# learning_rate_decay: 0.00002 # If update_learning_rate, decay learning rate by this much if steps have gone past start_decay_steps
# start_decay_steps: 10000 # Start decaying lr after X steps
# decay_steps: 4000 # Decay lr after every X steps
max_grad_norm: 0
label_smoothing: 0.1
#param_init: 0
param_init_glorot: true # Required for transformer. Default: false
normalization: "tokens" # Normalization method of the gradient.

# Model
encoder_type: transformer
decoder_type: transformer
position_encoding: true
enc_layers: 6
dec_layers: 6
heads: 8
hidden_size: 512 # RNN hidden size
word_vec_size: 512 # Word embedding size
enc_rnn_size: 512 # Here because the model doesn't work otherwise
dec_rnn_size: 512 # Equally useful compared to the line above it
transformer_ff: 2048 # Size of hidden transformer feed-forward, taken from Transformer paper
dropout_steps: [0]
dropout: [0.1]

# Attention
self_attn_type: scaled-dot
attention_dropout: [0.1]