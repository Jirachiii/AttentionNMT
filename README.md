# Performances of different attention functions in the task of Neural Machine Translation
This project was created to answer the research question: Which attention function can return the best translation model given the same amount of training?
Approach: Each attention function will be given the same architecture, hardware, dataset, and optimizer, we will observe their translation performance after being trained for the same number of iterations/epoches.

Further details can be found in 'ProjectReport.pdf'.
